{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import layers,optimizers,regularizers,losses \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import time\n",
    "from Data import load_ppi\n",
    "from BatchLoader import Build_batch_from_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"Data\\\\ppi\"\n",
    "\n",
    "SAMPLE_SIZES = [25, 10]\n",
    "INTERNAL_DIM = 128\n",
    "NEG_WEIGHT = 1.0\n",
    "BATCH_SIZE = 512\n",
    "NEG_SIZE = 20\n",
    "TRAINING_STEPS = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_batch(adj_lists, batch_size, sample_sizes, neg_size):\n",
    "    edges = np.array([(k, v) for k in adj_lists for v in adj_lists[k]])\n",
    "    nodes = np.array(list(adj_lists.keys()))\n",
    "    while True :\n",
    "        batch_edges = edges[np.random.randint(edges.shape[0],size=batch_size),:]\n",
    "        \n",
    "        batch = Build_batch_from_edges(batch_edges,nodes,adj_lists,sample_sizes,neg_size)\n",
    "        yield batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Features(tf.keras.layers.Layer):\n",
    "    def __init__(self,features_data):\n",
    "        super(Features,self).__init__()\n",
    "        self.features = tf.constant(features_data,dtype=tf.float32)\n",
    "    def call(self,nodes):\n",
    "        return tf.gather(self.features,nodes)\n",
    "\n",
    "\n",
    "class MeanAgg(tf.keras.layers.Layer):\n",
    "    def __init__(self,hu_shape,hv_shape,name,active = True):\n",
    "        super(MeanAgg,self).__init__()\n",
    "        self.w = self.add_weight(name=name+\"_w\",\n",
    "                                 shape=(hu_shape*2,hv_shape),\n",
    "                                 dtype=tf.float32,\n",
    "                                 trainable=True)\n",
    "    \n",
    "        self.active = active\n",
    "\n",
    "    def call(self,nodes_features,hu,hv,dif_mat):\n",
    "        hu_features = tf.gather(nodes_features,hu)\n",
    "        hv_features = tf.gather(nodes_features,hv)\n",
    "        agg = tf.matmul(dif_mat,hu_features)\n",
    "        \n",
    "        conct = tf.concat([agg,hv_features],1)\n",
    "        out = tf.matmul(conct,self.w)\n",
    "        if self.active:\n",
    "            out = tf.nn.relu(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class GraphSage(tf.keras.Model):\n",
    "    def __init__(self,data_features,num_layers,in_dim,active):\n",
    "        super(GraphSage,self).__init__()\n",
    "        self.input_layer = Features(data_features)\n",
    "        self.agg_layers = []\n",
    "        for i in range(1,num_layers+1):\n",
    "            name = \"agg_\"+str(i)\n",
    "            input_dim = in_dim if i>1 else  data_features.shape[-1]\n",
    "            isactive = active if i == num_layers else True\n",
    "            agg_layer = MeanAgg(input_dim,in_dim,name= name,active=isactive)\n",
    "            self.agg_layers.append(agg_layer)\n",
    "    \n",
    "    def call(self,batch):\n",
    "        x = self.input_layer(tf.squeeze(batch.src_nodes))\n",
    "        for agg_layer in self.agg_layers:\n",
    "            x = agg_layer(x,\n",
    "                          batch.dst_src_src.pop(),\n",
    "                          batch.dst_src_dst.pop(),\n",
    "                          batch.dif_mats.pop(),\n",
    "                          )\n",
    "        return x\n",
    "    \n",
    "class GraphSageSupervise(GraphSage):\n",
    "    def __init__(self,data_feats,in_dim,num_layers,num_classes):\n",
    "        super().__init__(data_feats,num_layers,in_dim,True)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes,tf.nn.softmax,use_bias=False,name=\"classifier\")\n",
    "\n",
    "    def call(self,batch):\n",
    "        return self.classifier(super().call(batch))\n",
    "\n",
    "\n",
    "def unsupervise_loss(embeddingA,embeddingB,embeddingN,neg_weight):\n",
    "    pos_emb = tf.reduce_sum(tf.multiply(embeddingA,embeddingB),axis=1)\n",
    "    neg_emb = tf.matmul(embeddingA,tf.transpose(embeddingN))\n",
    "\n",
    "    pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(tf.ones_like(pos_emb),pos_emb)\n",
    "    neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(neg_emb),neg_emb)\n",
    "\n",
    "    neg_loss_w = tf.multiply(neg_weight,tf.reduce_sum(neg_loss))\n",
    "    batch_loss = tf.add(neg_loss_w,tf.reduce_sum(pos_loss))\n",
    "\n",
    "    return tf.divide(batch_loss,embeddingA.shape[0])\n",
    "\n",
    "\n",
    "class GraphSageUnsupervise(GraphSage):\n",
    "    def __init__(self,data_feats,in_dim,num_layers,neg_weight):\n",
    "        super().__init__(data_feats,num_layers,in_dim,False)\n",
    "        self.neg_weight = neg_weight\n",
    "    def call(self,batch):\n",
    "        x=super().call(batch)\n",
    "        embedding = tf.math.l2_normalize(x)\n",
    "        self.add_loss(\n",
    "            unsupervise_loss(\n",
    "                tf.gather(embedding,batch.dst2batchA),\n",
    "                tf.gather(embedding,batch.dst2batchB),\n",
    "                tf.boolean_mask(embedding,batch.dst2batchN),\n",
    "                self.neg_weight\n",
    "            )\n",
    "        )\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    num_nodes,feat_data,adj_lists = load_ppi() \n",
    "    batch_generator = generate_batch(adj_lists,BATCH_SIZE,SAMPLE_SIZES,NEG_SIZE)\n",
    "\n",
    "    model = GraphSageUnsupervise(feat_data,INTERNAL_DIM,len(SAMPLE_SIZES),NEG_WEIGHT)\n",
    "    optimizer = optimizers.Adam(LR)\n",
    "\n",
    "    times = []\n",
    "    i=1\n",
    "    for batch in islice(batch_generator,0,TRAINING_STEPS):\n",
    "        \n",
    "        start_t = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            _ = model(batch)\n",
    "            loss = model.losses[0]\n",
    "        grads = tape.gradient(loss,model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "        end_t = time.time()\n",
    "        times.append(start_t-end_t)\n",
    "        print(f\"Loss in step {i} is :{loss.numpy()}\")\n",
    "        i+=1\n",
    "    print(f\"average batch time is {np.mean(times)}\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
